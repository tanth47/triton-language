{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2aa248c",
   "metadata": {},
   "source": [
    "\n",
    "# Optimizing Triton Kernels on AMD MI300X (ROCm)\n",
    "\n",
    "This notebook is a hands‑on companion for optimizing Triton kernels on AMD Instinct™ MI300X GPUs under ROCm.\n",
    "It follows the themes from AMD’s official guidance on **Triton kernel optimization** and adapts them into practical, reproducible experiments you can run on your MI300X box.\n",
    "\n",
    "\"*Broadly, Triton kernel optimization is similar to HIP and CUDA kernel optimization.*\" - AMD ROCm Doc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec1313d",
   "metadata": {},
   "source": [
    "## 0. Environment check\n",
    "\n",
    "1. rocm version: 6.2.0\n",
    "2. torch version: 2.4.1+rocm6.0\n",
    "3. triton version: 3.4.0\n",
    "4. HIP version: 6.2.41133-dd7f95766\n",
    "5. Hardware: MI300X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b273dbfe",
   "metadata": {},
   "source": [
    "## 1. Memory Access Efficiency\n",
    "\n",
    "Modern accelerators/GPU cores expose a memory hierarchy:\n",
    "\n",
    "- **Global memory** — large capacity, **high latency**.\n",
    "- **Local Data Share (LDS) / Shared memory** — much **lower latency**, but **limited size**.\n",
    "- **Registers** — **fastest**, **smallest**.\n",
    "\n",
    "**Guidelines**\n",
    "- Minimize round-trips to **global memory** (load/store as few times as possible).\n",
    "- When multiple threads in a workgroup need the same data, **stage it in LDS** once, then let threads reuse it from there.\n",
    "\n",
    "![GPU Memory Hierarchy](./imgs/mem_hierarchy.png)\n",
    "\n",
    "*Image source: [Simon Oz — GPU Programming Ep. 6](https://www.youtube.com/watch?v=Zrbw0zajhJM).*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600d28b6",
   "metadata": {},
   "source": [
    "## 2. Know Your Hardware, Win Your Kernel\n",
    "\n",
    "A solid grasp of the hardware leads to better heuristics for maximizing utilization.  \n",
    "For **AMD MI300X**, here’s a practical rule-of-thumb:\n",
    "\n",
    "![MI300X-CU](./imgs/mi300x_CU.png)\n",
    "\n",
    "According to the [hardware spec](https://rocm.docs.amd.com/en/docs-6.1.1/reference/gpu-arch-specs.html), **MI300X** has **4 SIMD units per CU** and a **wavefront (warp) size of 64**.  \n",
    "Implications:\n",
    "\n",
    "- **Block size ≥ 256 threads (4 wavefronts)** → you can keep all 4 SIMDs in a CU busy at once.\n",
    "- With **304 CUs**, aim for a grid with **≥ 1024 workgroups** (≈ **3–4 workgroups per CU**) to increase latency hiding and keep the device saturated.\n",
    "\n",
    "To further raise parallelism and utilization, design algorithms that expose more independent work.  \n",
    "For GEMM, a common tactic is **larger split-K**: partition the K dimension so more partial products can run across additional CUs, improving concurrency and throughput (with a final reduction to combine partial results)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72774ff",
   "metadata": {},
   "source": [
    "## 3. IR analysis\n",
    "\n",
    "\"*In Triton, there are several layouts including blocked, shared, sliced, and MFMA.*\"\n",
    "\n",
    "From Triton GPU IR (TTGIR) you can infer **where** a value lives (global → registers → LDS/shared) and **how** it’s laid out.  \n",
    "If you’re new to TTGIR, see my notebook that **demystifies Triton’s compilation stages**: [`triton_compilation_stages.ipynb`](./triton_compilation_stages.ipynb).\n",
    "\n",
    "Below is a snippet from a FlashAttention **decode** kernel that **dequantizes int4 key/value** to `f16`.\n",
    "```\n",
    "    %190 = tt.load %189 {cache = 1 : i32, evict = 1 : i32, isVolatile =\n",
    "    false} : tensor<1x64xi32, #blocked6> loc(#loc159)\n",
    "\n",
    "    %266 = arith.andi %190, %cst_28 : tensor<1x64xi32, #blocked6>\n",
    "    loc(#loc250)\n",
    "\n",
    "    %267 = arith.trunci %266 : tensor<1x64xi32, #blocked6> to\n",
    "    tensor<1x64xi16, #blocked6> loc(#loc251)\n",
    "\n",
    "    %268 = tt.bitcast %267 : tensor<1x64xi16, #blocked6> -> tensor<1x64xf16,\n",
    "    #blocked6> loc(#loc252)\n",
    "\n",
    "    %269 = triton_gpu.convert_layout %268 : (tensor<1x64xf16, #blocked6>) ->\n",
    "    tensor<1x64xf16, #shared1> loc(#loc252)\n",
    "\n",
    "    %270 = tt.trans %269 : (tensor<1x64xf16, #shared1>) -> tensor<64x1xf16,\n",
    "    #shared2> loc(#loc194)\n",
    "\n",
    "    %276 = triton_gpu.convert_layout %270 : (tensor<64x1xf16, #shared2>) ->\n",
    "    tensor<64x1xf16, #blocked5> loc(#loc254)\n",
    "\n",
    "    %293 = arith.mulf %276, %cst_30 : tensor<64x1xf16, #blocked5>\n",
    "    loc(#loc254)\n",
    "\n",
    "    %295 = arith.mulf %292, %294 : tensor<64x32xf16, #blocked5> loc(#loc264)\n",
    "\n",
    "    %297 = arith.addf %295, %296 : tensor<64x32xf16, #blocked5> loc(#loc255)\n",
    "\n",
    "    %298 = triton_gpu.convert_layout %297 : (tensor<64x32xf16, #blocked5>)\n",
    "    -> tensor<64x32xf16, #shared1> loc(#loc255)\n",
    "\n",
    "    %299 = tt.trans %298 : (tensor<64x32xf16, #shared1>) ->\n",
    "    tensor<32x64xf16, #shared2> loc(#loc196)\n",
    "\n",
    "    %300 = triton_gpu.convert_layout %299 : (tensor<32x64xf16, #shared2>) ->\n",
    "    tensor<32x64xf16, #triton_gpu.dot_op<{opIdx = 1, parent = #mfma, kWidth\n",
    "    = 4}>> loc(#loc197)\n",
    "```\n",
    "\n",
    "### What this IR does\n",
    "\n",
    "1) **Load + scalar ops in registers**  \n",
    "   Load `i32` from **global** into registers; mask, truncate, and reinterpret to `f16`:\n",
    "    ```\n",
    "    %190, %266, %267, %268\n",
    "    ```\n",
    "\n",
    "2) **Stage to LDS for cross-thread transpose**  \n",
    "   Move to **shared (LDS)** and **transpose** so subsequent accesses are coalesced and bank-friendly:\n",
    "    ```\n",
    "    %269, %270\n",
    "    ```\n",
    "\n",
    "3) **Compute in registers, restage to LDS**  \n",
    "   Bring back to a blocked/register view, apply elementwise mul/add, then **store back to LDS**:\n",
    "    ```\n",
    "    %276, %293, %295, %297 -> %298\n",
    "    ```\n",
    "\n",
    "   This is the classic “**stage in shared, transpose for coalescing/bank access**” pattern.  \n",
    "   More: [Memory Coalescing](https://modal.com/gpu-glossary/perf/memory-coalescing), [Bank Conflict](https://modal.com/gpu-glossary/perf/bank-conflict)\n",
    "\n",
    "4) **Prepare dot-operand layout (MFMA)**  \n",
    "   Transpose again in **shared**, then convert to the **dot-op (MFMA) layout** for matrix ops:\n",
    "    ```\n",
    "    %299 -> %300\n",
    "    ```\n",
    "\n",
    "**Takeaway:** LDS is used **twice**—first to enable a safe/efficient **transpose**, then to bridge from a **blocked** layout into an **MFMA-friendly dot operand** layout, minimizing global traffic while enabling coalesced loads and conflict-free shared accesses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ae62e6",
   "metadata": {},
   "source": [
    "## 4. Assembly Analysis\n",
    "\n",
    "### Global memory: vectorized loads\n",
    "- Prefer **`global_load_dwordx4`** (per-lane 16 B = 4×32-bit) — especially inside loops.  \n",
    "  Vectorizing reduces LD instruction count while remaining coalesced across lanes.\n",
    "\n",
    "### LDS (shared) width: b128 vs b64\n",
    "- **`ds_read_b128` / `ds_write_b128`**: 16 B per lane (preferred).\n",
    "- **`ds_read_b64` / `ds_write_b64`**: 8 B per lane (fallback when alignment/aliasing/layout prevents b128).\n",
    "- **Rule of thumb:** design shared layouts so **b128** is *legal and bank-friendly*; expect ~2× fewer LDS ops vs b64.\n",
    "\n",
    "### `s_waitcnt`: fencing only when needed\n",
    "- `s_waitcnt lgkmcnt(n)`: waits on **LDS/GDS/const/msg** ops.  \n",
    "  `lgkmcnt(0)` ⇒ all such accesses complete; `1` ⇒ allow one still in flight, etc.\n",
    "- `s_waitcnt vmcnt(n)`: waits on **vector (global) memory** ops.  \n",
    "  Same semantics: fence only as much as required before first use.\n",
    "\n",
    "**Latency hiding pattern (skeleton):** issue memory early → do unrelated compute while it’s in flight → `s_waitcnt` right before first use.  \n",
    "    Skeleton in AMD-ish pseudocode:\n",
    "\n",
    "```\n",
    "    ; ---- Prefetch next tile from HBM (global) ----\n",
    "    global_load_dwordx4 v[a0:a3], [rptrA]     ; A_next\n",
    "    global_load_dwordx4 v[b0:b3], [rptrB]     ; B_next\n",
    "\n",
    "    ; ---- Do math on the current tile while the loads are in flight ----\n",
    "    v_fma_f32 acc0, a_cur0, b_cur0, acc0\n",
    "    v_fma_f32 acc1, a_cur1, b_cur1, acc1\n",
    "    ; ... more independent VALU / MFMA ...\n",
    "\n",
    "    ; ---- Only now, fence the loads before consuming them ----\n",
    "    s_waitcnt vmcnt(0)                         ; A_next/B_next ready\n",
    "\n",
    "    ; Optionally stage into LDS using wide ops\n",
    "    ds_write_b128 [ldsA + off], v[a0:a3]\n",
    "    ds_write_b128 [ldsB + off], v[b0:b3]\n",
    "    s_waitcnt lgkmcnt(0)                       ; if you must read back immediately\n",
    "\n",
    "    ; Read back (conflict-free layout) and continue compute\n",
    "    ds_read_b128  v[a_cur0:a_cur3], [ldsA + off2]\n",
    "    ds_read_b128  v[b_cur0:b_cur3], [ldsB + off3]\n",
    "    s_waitcnt lgkmcnt(0)                       ; before first use\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Practical checklist\n",
    "- **Vectorize** global loads/stores (target **`dwordx4` / 16 B per lane**).\n",
    "- **Prefer LDS b128**; if you see **b64**, investigate alignment, aliasing, or bank mapping.\n",
    "- **Fence late**: place `s_waitcnt` immediately before data consumption, not earlier.\n",
    "- **Trace inefficiencies**: if codegen looks suboptimal, walk back through **LLVM IR → TTGIR → TTIR**.  \n",
    "  Enable MLIR/LLVM dumps to spot which pass/layout choice caused the issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f668e45",
   "metadata": {},
   "source": [
    "## 5. Kernel Occupancy\n",
    "\n",
    "Occupancy = how many **workgroups** (and thus **wavefronts**) can reside on a **Compute Unit (CU)** concurrently, given limits from **LDS**, **wavefront slots**, and **VGPRs**. Below is a practical, reproducible way to estimate it for MI300-class GPUs.\n",
    "\n",
    "1. LDS-limited workgroups per CU\n",
    "    * Get the allocated LDS used per workgroup following the steps (for example, L for the kernel).\n",
    "        * export MLIR_ENABLE_DUMP=1\n",
    "        * rm -rf ~/.triton/cache\n",
    "        * python kernel.py | | grep \"triton_gpu.shared = \" | tail -n 1\n",
    "        * You should see something like triton_gpu.shared = 65536, indicating 65536 bytes of LDS are allocated for the kernel.\n",
    "    *  A CU has 64 KiB LDS total. If each workgroup needs L bytes: `occ_lds = floor(65536 / L)`\n",
    "\n",
    "2. Warp-limites workgroups per CU\n",
    "    * Get number of waves per workgroup using the following steps (for example, nW).\n",
    "        * export MLIR_ENABLE_DUMP=1\n",
    "        * rm -rf ~/.triton/cache\n",
    "        * python kernel.py | | grep \"triton_gpu.num-warps \" | tail -n 1\n",
    "        * You should see something like “triton_gpu.num-warps\" = 8, indicating 8 waves per workgroup.\n",
    "    * A CU has 32 warps total. If each workgroup need nW warps: `occ_warp = floor(32 / nW)`\n",
    "\n",
    "3. VGPR-limited workgroups per CU\n",
    "    * Get the VGPR count per thread, search for `.vgpr_count` in the ISA (for example, N).\n",
    "    * Compute occupancy limited by VGPR based on N according to the following table. For example, waves per EU (SIMD) as `occ_vgpr_eu`\n",
    "\n",
    "    ![VGPRs Occupancy Table](./imgs/vgpr_occ_table.png)\n",
    "    \n",
    "    * The `occ_vgpr_cu = occ_vgpr_eu * 4`, in CDNA2/3 we have 4 SIMD per CU. A CU can only contains up to `occ_vgpr_cu` due to VGPR pressure, a workgroup consume `nW` warps -> `occ_vgpr = floor(occ_vgpr_cu / nW)`\n",
    "\n",
    "The true `occupancy = min(occ_lds, occ_vgpr)`. In practice, the `occ_warp` is always >= `occ_vgpr`.\n",
    "\n",
    "Optional read: The VGPRs Occupancy Table is provided by AMD, but you can calculate it by hands. According to MI300X spec, we have 512 KiB VGPR file in total -> 128 KiB each SIMD. We have the `vgpr_count` per thread -> each warp consume `vgpr_used = vgpr_count * warp_size * 4` bytes (each register is 32 bit). -> `occ_vgpr_eu = floor (128K / vgpr_used)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2230112c",
   "metadata": {},
   "source": [
    "## 6. Auto-tunable Kernel Configurations and Environment Variables\n",
    "\n",
    "### Triton reserved keywords\n",
    "\n",
    "These are common Triton meta-parameters you’ll want to tune.\n",
    "\n",
    "\n",
    "#### 1. `BLOCK_M`, `BLOCK_N`, `BLOCK_K`\n",
    "\n",
    "Tile sizes for GEMM-like workloads. They control the **memory-to-compute ratio** and **grid-level parallelism**:\n",
    "\n",
    "- Make tiles **large enough** to:\n",
    "  - Improve arithmetic intensity.\n",
    "  - Reuse data in registers/LDS efficiently.\n",
    "- But also **small enough** to:\n",
    "  - Launch many workgroups.\n",
    "  - Keep all CUs busy and hide latency.\n",
    "\n",
    "You typically explore a grid of `(BLOCK_M, BLOCK_N, BLOCK_K)` configs in the autotuner.\n",
    "\n",
    "\n",
    "#### 2. `num_stages = n`\n",
    "\n",
    "Controls the number of pipeline stages (software pipelining / prefetching behavior). AMD’s guidelines:\n",
    "\n",
    "- Kernels with **a single GEMM** → `num_stages = 0`.\n",
    "- Kernels with **two GEMMs fused** (e.g., Flash Attention, or any 2-GEMM fusion) → `num_stages = 1`.\n",
    "- Kernels with **one GEMM fused with a non-GEMM op** (e.g., GEMM + ReLU) → `num_stages = 0`.\n",
    "- Kernels with **no GEMM** → `num_stages = 1`.\n",
    "\n",
    "These are good defaults; fine-tuning is still workload-dependent.\n",
    "\n",
    "\n",
    "#### 3. `matrix_instr_nonkdim`\n",
    "\n",
    "Experimental parameter for FlashAttention-like kernels that selects the MFMA instruction shape:\n",
    "\n",
    "- `matrix_instr_nonkdim = 16` → use `mfma_16x16`.\n",
    "- `matrix_instr_nonkdim = 32` → use `mfma_32x32`.\n",
    "\n",
    "On AMD MI300X, **`mfma_16x16` usually outperforms `mfma_32x32`** for GEMM kernels, even with large tiles.\n",
    "\n",
    "\n",
    "#### 4. `waves_per_eu = n`\n",
    "\n",
    "Hints the backend to **cap VGPR usage** so that a desired number of waves per EU (SIMD) can be resident.\n",
    "\n",
    "Useful when:\n",
    "\n",
    "- **Occupancy is VGPR-limited**, and\n",
    "- Your current VGPR usage is just **slightly above** a VGPR occupancy boundary (see the VGPRs Occupancy Table).\n",
    "\n",
    "Example:\n",
    "\n",
    "- Each EU (SIMD) has **512 VGPRs**, allocated in **chunks of 16**.\n",
    "- Suppose `vgpr_count = 170` is reported:\n",
    "  - Actual allocation is rounded to 176.\n",
    "  - `176 × 3 > 512` → only **2 waves per EU** fit.\n",
    "- If you set `waves_per_eu = 3`, the LLVM backend tries to reduce VGPR usage enough to allow **3 waves per EU**, potentially improving occupancy.\n",
    "\n",
    "\n",
    "### Environment variable: `OPTIMIZE_EPILOGUE`\n",
    "\n",
    "#### What is the “epilogue” in this context?\n",
    "\n",
    "For an MFMA GEMM kernel, the high-level flow:\n",
    "\n",
    "1. **Main loop**\n",
    "   - Load A/B tiles.\n",
    "   - Perform MFMA → accumulate into registers in **MFMA layout** (`dot_op`).\n",
    "2. **Epilogue**\n",
    "   - Take the accumulator tile in MFMA’s per-lane layout.\n",
    "   - Optionally apply **bias / activation / etc.**\n",
    "   - **Repack** into a normal (blocked) layout.\n",
    "   - Store the final result to **global memory**.\n",
    "\n",
    "The **repacking** usually happens via `convert_layout` (often using LDS as a staging buffer):  \n",
    "`dot_op → [LDS] → blocked`.  \n",
    "Padding is added to avoid LDS bank conflicts, but this **increases LDS usage** and can **reduce occupancy**.\n",
    "\n",
    "\n",
    "#### Behavior of `OPTIMIZE_EPILOGUE`\n",
    "\n",
    "- **`OPTIMIZE_EPILOGUE=0` (default)**  \n",
    "  - Convert MFMA accumulators to a blocked layout.\n",
    "  - Enables fully vectorized stores, e.g. `global_store_dwordx4`.\n",
    "  - Cost: extra LDS usage + extra data movement/ops.\n",
    "\n",
    "- **`OPTIMIZE_EPILOGUE=1`**  \n",
    "  - Store MFMA results **directly in MFMA layout**.\n",
    "  - Global stores may be less pretty / less vectorized.\n",
    "  - Benefit: lower LDS usage, fewer instructions, often **better occupancy**.\n",
    "  - In practice, the impact on runtime is usually **small or positive** despite less perfect stores.\n",
    "\n",
    "**In short:**\n",
    "\n",
    "- `OPTIMIZE_EPILOGUE=0`:  \n",
    "  - Pay LDS + extra ops to get **ideal, vectorized global stores**.\n",
    "\n",
    "- `OPTIMIZE_EPILOGUE=1`:  \n",
    "  - Skip the expensive repack;  \n",
    "  - **Store MFMA layout directly**, accept slightly worse store patterns,  \n",
    "  - Gain **occupancy** and reduce instruction count.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3154398",
   "metadata": {},
   "source": [
    "## References\n",
    "- AMD ROCm Docs — [Optimizing Triton kernels](https://rocm.docs.amd.com/en/docs-6.1.1/how-to/llm-fine-tuning-optimization/optimizing-triton-kernel.html)\n",
    "- GPU Glossary - [Performance](https://modal.com/gpu-glossary/perf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "triton-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
