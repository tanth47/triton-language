{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6dc5d1e",
   "metadata": {},
   "source": [
    "# Triton Compilation Stages — Hands-On Notebook\n",
    "\n",
    "[Triton](https://openai.com/index/triton/) offers a high-level, Python-based way to write efficient GPU code.  \n",
    "In this notebook, we walk through how a Triton program is compiled, focusing on the intermediate representations (IR) you can inspect along the way.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4624a38",
   "metadata": {},
   "source": [
    "## Triton Language\n",
    "\n",
    "As a running example, we follow (with minor tweaks) the Triton vector-add tutorial.  \n",
    "The kernel and a small helper are defined below.\n",
    "\n",
    " ```\n",
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "def is_cuda():\n",
    "    return triton.runtime.driver.active.get_current_target().backend == \"cuda\"\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def add_kernel(\n",
    "    x_ptr, y_ptr, z_ptr,\n",
    "    N: tl.constexpr,\n",
    "    BLOCK_SIZE: tl.constexpr\n",
    "):\n",
    "    blockidx = tl.program_id(axis=0)\n",
    "    offsets = blockidx * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n",
    "    mask = offsets < N\n",
    "    x = tl.load(x_ptr + offsets, mask=mask, other=0.0)\n",
    "    y = tl.load(y_ptr + offsets, mask=mask, other=0.0)\n",
    "\n",
    "    z = x + y\n",
    "\n",
    "    tl.store(z_ptr + offsets, z, mask=mask)\n",
    "\n",
    "\n",
    "def add(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:\n",
    "    assert x.shape == y.shape, \"Input tensors must have the same shape\"\n",
    "    assert x.is_cuda and y.is_cuda, \"Input tensors must be on CUDA device\"\n",
    "    assert x.dtype == y.dtype, \"Input tensors must have the same dtype\"\n",
    "\n",
    "    z = torch.empty_like(x)\n",
    "    N = x.numel()\n",
    "    BLOCK_SIZE = 1024\n",
    "    grid = (N + BLOCK_SIZE - 1) // BLOCK_SIZE\n",
    "    grid = (grid,)\n",
    "\n",
    "    triton_kernel = add_kernel[grid](\n",
    "        x_ptr=x,\n",
    "        y_ptr=y,\n",
    "        z_ptr=z,\n",
    "        N=N,\n",
    "        BLOCK_SIZE=BLOCK_SIZE,\n",
    "    )\n",
    "    \n",
    "    # Save compilation stages - some of the stages identified here are specific to NVIDIA devices:\n",
    "    with open('triton_IR.txt', 'w') as f:\n",
    "        print(triton_kernel.asm['ttir'], file=f)\n",
    "    with open('triton_TTGIR.txt', 'w') as f:\n",
    "        print(triton_kernel.asm['ttgir'], file=f)\n",
    "    with open('triton_LLVMIR.txt', 'w') as f:\n",
    "        print(triton_kernel.asm['llir'], file=f)\n",
    "    if is_cuda():\n",
    "        with open('triton_PTX.ptx', 'w') as f:\n",
    "            print(triton_kernel.asm['ptx'], file=f)\n",
    "        with open('triton_cubin.txt', 'w') as f:\n",
    "            print(triton_kernel.asm['cubin'], file=f)\n",
    "    else:\n",
    "        with open('triton_AMDGCN.ptx', 'w') as f:\n",
    "            print(triton_kernel.asm['amdgcn'], file=f)\n",
    "        with open('triton_hsaco.txt', 'w') as f:\n",
    "            print(triton_kernel.asm['hsaco'], file=f)\n",
    "    \n",
    "    return z\n",
    "```\n",
    "\n",
    "Key points:\n",
    "\n",
    "1. The vector-add kernel is decorated with `@triton.jit`. Functions marked with `@triton.jit` are compiled by Triton and lowered through multiple stages.\n",
    "2. The helper function `add` allocates the output tensor, computes an appropriate GPU grid, and captures intermediate IR artifacts during compilation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f6e51a",
   "metadata": {},
   "source": [
    "## Triton Compiler\n",
    "\n",
    "We’ll focus on how a Triton kernel is lowered to device-specific assembly through a sequence of stages (see the figure below).\n",
    "\n",
    "![Triton Compilation Stages](./imgs/triton_compilation_stages.png)\n",
    "\n",
    "**High level flow**\n",
    "\n",
    "1. A Python Triton kernel (your `@triton.jit` function) enters the compiler.\n",
    "\n",
    "**Top box — “MLIR” (Triton front/middle end)**\n",
    "\n",
    "2. **Triton-IR (TTIR)** — Produced by walking the kernel’s AST. It’s unoptimized, machine-independent, tile-oriented, and implemented as an MLIR dialect.  \n",
    "3. **Triton-GPU-IR (TTGIR)** — Still MLIR, but GPU-aware (NVIDIA or AMD). Triton applies GPU-specific optimizations here.  \n",
    "4. **LLVM IR** — TTGIR is lowered to standard LLVM IR.\n",
    "\n",
    "**Bottom box — “LLVM” (back end)**\n",
    "\n",
    "5. **Device Assembly** — LLVM’s targets emit ISA text: PTX (NVIDIA) or AMDGCN (AMD).  \n",
    "6. **Loadable Code Objects** — PTX → assembled by `ptxas` into a **cubin**; AMDGCN → linked into an **hsaco** (HSA code object).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb533143",
   "metadata": {},
   "source": [
    "## Vector Addition - TTIR\n",
    "\n",
    "Let's try to ingest Vector Addition's TTIR. Here's the Triton IR:\n",
    "```\n",
    "module {\n",
    "  tt.func public @add_kernel(\n",
    "    %arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32} ..., \n",
    "    %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32} ..., \n",
    "    %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32} ...\n",
    "    ) \n",
    "  \n",
    "  attributes {noinline = false} {\n",
    "    %cst = arith.constant dense<0.000000e+00> : tensor<1024xf32> loc(#loc1)\n",
    "    %cst_0 = arith.constant dense<1024> : tensor<1024xi32> loc(#loc1)\n",
    "    %c1024_i32 = arith.constant 1024 : i32 loc(#loc1)\n",
    "    %0 = tt.get_program_id x : i32 loc(#loc2)\n",
    "    %1 = arith.muli %0, %c1024_i32 : i32 loc(#loc3)\n",
    "    %2 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32> loc(#loc4)\n",
    "    %3 = tt.splat %1 : i32 -> tensor<1024xi32> loc(#loc5)\n",
    "    %4 = arith.addi %3, %2 : tensor<1024xi32> loc(#loc5)\n",
    "    %5 = arith.cmpi slt, %4, %cst_0 : tensor<1024xi32> loc(#loc6)\n",
    "    %6 = tt.splat %arg0 : !tt.ptr<f32> -> tensor<1024x!tt.ptr<f32>> loc(#loc7)\n",
    "    %7 = tt.addptr %6, %4 : tensor<1024x!tt.ptr<f32>>, tensor<1024xi32> loc(#loc7)\n",
    "    %8 = tt.load %7, %5, %cst : tensor<1024x!tt.ptr<f32>> loc(#loc8)\n",
    "    %9 = tt.splat %arg1 : !tt.ptr<f32> -> tensor<1024x!tt.ptr<f32>> loc(#loc9)\n",
    "    %10 = tt.addptr %9, %4 : tensor<1024x!tt.ptr<f32>>, tensor<1024xi32> loc(#loc9)\n",
    "    %11 = tt.load %10, %5, %cst : tensor<1024x!tt.ptr<f32>> loc(#loc10)\n",
    "    %12 = arith.addf %8, %11 : tensor<1024xf32> loc(#loc11)\n",
    "    %13 = tt.splat %arg2 : !tt.ptr<f32> -> tensor<1024x!tt.ptr<f32>> loc(#loc12)\n",
    "    %14 = tt.addptr %13, %4 : tensor<1024x!tt.ptr<f32>>, tensor<1024xi32> loc(#loc12)\n",
    "    tt.store %14, %12, %5 : tensor<1024x!tt.ptr<f32>> loc(#loc13)\n",
    "    tt.return loc(#loc14)\n",
    "  } loc(#loc)\n",
    "} loc(#loc)\n",
    "```\n",
    "Mapping from Triton Kernel to TTIR:\n",
    "1. Constants for Block:\n",
    "```\n",
    "    %cst = arith.constant dense<0.000000e+00> : tensor<1024xf32> loc(#loc1)\n",
    "    %cst_0 = arith.constant dense<1024> : tensor<1024xi32> loc(#loc1)\n",
    "    %c1024_i32 = arith.constant 1024 : i32 loc(#loc1)\n",
    "```\n",
    "2. Which Block am I?\n",
    "```\n",
    "    %0 = tt.get_program_id x : i32 loc(#loc2)\n",
    "    %1 = arith.muli %0, %c1024_i32 : i32 loc(#loc3) # base = pid_x * 1024\n",
    "```\n",
    "3. Per-lane indices inside the CTA:\n",
    "```\n",
    "    %2 = tt.make_range {end = 1024 : i32, start = 0 : i32} : tensor<1024xi32> loc(#loc4)     # [0,1,...,1023] in blocked layout\n",
    "    %3 = tt.splat %1 : i32 -> tensor<1024xi32> loc(#loc5)                                    # broadcast base to all lanes\n",
    "    %4 = arith.addi %3, %2 : tensor<1024xi32> loc(#loc5)                                     # global indices for this CTA’s 1024 elems\n",
    "    %5 = arith.cmpi slt, %4, %cst_0 : tensor<1024xi32> loc(#loc6)                            # mask = (idx < N)             \n",
    "```\n",
    "4. Compute x, y addresses then load with mask and elementwise add\n",
    "```\n",
    "    %6 = tt.splat %arg0 : !tt.ptr<f32> -> tensor<1024x!tt.ptr<f32>> loc(#loc7)              # broadcast x base ptr\n",
    "    %7 = tt.addptr %6, %4 : tensor<1024x!tt.ptr<f32>>, tensor<1024xi32> loc(#loc7)          # x_ptr + idx\n",
    "    %8 = tt.load %7, %5, %cst : tensor<1024x!tt.ptr<f32>> loc(#loc8)                        # masked load x (else 0.0)                    \n",
    "\n",
    "\n",
    "    %9 = tt.splat %arg1 : !tt.ptr<f32> -> tensor<1024x!tt.ptr<f32>> loc(#loc9)\n",
    "    %10 = tt.addptr %9, %4 : tensor<1024x!tt.ptr<f32>>, tensor<1024xi32> loc(#loc9)\n",
    "    %11 = tt.load %10, %5, %cst : tensor<1024x!tt.ptr<f32>> loc(#loc10)                     # masked load y\n",
    "\n",
    "    %12 = arith.addf %8, %11 : tensor<1024xf32> loc(#loc11)                # x + y\n",
    "```\n",
    "5.  Compute out addresses and store with mask\n",
    "```\n",
    "    %13 = tt.splat %arg2 : !tt.ptr<f32> -> tensor<1024x!tt.ptr<f32>> loc(#loc12)\n",
    "    %14 = tt.addptr %13, %4 : tensor<1024x!tt.ptr<f32>>, tensor<1024xi32> loc(#loc12)\n",
    "    tt.store %14, %12, %5 : tensor<1024x!tt.ptr<f32>> loc(#loc13)                          # masked store\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d369cb",
   "metadata": {},
   "source": [
    "## Vector Addition — TTGIR\n",
    "\n",
    "**Environment**\n",
    "\n",
    "1. triton 3.4.0  \n",
    "2. ROCm 6.2.0  \n",
    "3. AMD MI300X  \n",
    "4. torch 2.4.1+rocm6.0\n",
    "\n",
    "With the above setup, the simple vector addition has the following Triton GPU IR snippet with lines omitted for clarity:\n",
    "\n",
    "```\n",
    "#blocked = #triton_gpu.blocked<{sizePerThread = [4], threadsPerWarp = [64], warpsPerCTA = [4], order = [0]}>\n",
    "module attributes {\n",
    "  \"triton_gpu.num-ctas\" = 1 : i32, \n",
    "  \"triton_gpu.num-warps\" = 4 : i32, \n",
    "  triton_gpu.target = \"hip:gfx942\", \n",
    "  \"triton_gpu.threads-per-warp\" = 64 : i32\n",
    "} {\n",
    "  tt.func public @add_kernel(\n",
    "    %arg0: !tt.ptr<f32> \n",
    "    {tt.divisibility = 16 : i32} \n",
    "    %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32} \n",
    "    %arg2: !tt.ptr<f32> {tt.divisibility = 16 : i32} \n",
    "  ) \n",
    "    attributes {noinline = false} {\n",
    "    ...\n",
    "    %8 = tt.load %7, %5, %cst : tensor<1024x!tt.ptr<f32>, #blocked> loc(#loc8)\n",
    "    ...\n",
    "    %11 = tt.load %10, %5, %cst : tensor<1024x!tt.ptr<f32>, #blocked> loc(#loc10)\n",
    "    %12 = arith.addf %8, %11 : tensor<1024xf32, #blocked> loc(#loc11)\n",
    "    ...\n",
    "    tt.store %14, %12, %5 : tensor<1024x!tt.ptr<f32>, #blocked> loc(#loc13)\n",
    "    ...\n",
    "  } loc(#loc)\n",
    "} loc(#loc)\n",
    "```\n",
    "\n",
    "**Header & layout breakdown**\n",
    "\n",
    "\n",
    "With `warpsPerCTA = [4]` -> `numwarps=4` -> Each block has 4 * 64 (warpSize/threadsPerWarp) = 256 threads. And each thread handle 4 elements (sizePerThread) -> BlockSize = 256 * 4 = 1024 (`blocked` is a type of triton layout, we have blocked, sliced, shared, mfma, but we will talk about it later in Optimizing Triton Kernel notebook). \n",
    "\n",
    "\n",
    "At this stage, some of the hardware specific information is included. For example, the compute capability is included along with details on how the tensors are distributed to cores and warps. \n",
    "In this example, the tensors are represented as a `blocked` layout. In this encoding, each warp owns a contiguous portion of the tensor. Currently, other possible memory optimizations include layouts such as:\n",
    "1. slice - restructures and distributes a tensor along a dimension\n",
    "2. dot_op - optimized layout for block matrix product\n",
    "3. shared - indicates GPU shared memory\n",
    "4. nvidia_mma - produced by NVIDIA tensor cores\n",
    "5. amd_mfma - produced by AMD MFMA matrix core\n",
    "6. amd_wmma - produced by AMD WMMA matrix core\n",
    "\n",
    "Note: As announced at the recent Triton conference, this layout representation will transition to a new linear layout to unify layouts within and across backends. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a151377b",
   "metadata": {},
   "source": [
    "## Vector Addition — LLVM IR\n",
    "\n",
    "TTGIR is lowered to LLVM IR, LLVM’s standard representation.  \n",
    "Today, Triton supports NVIDIA and AMD through third-party backends; support for other devices is being actively developed by the OSS community.\n",
    "\n",
    "Below is an LLVM IR snippet for the vector-add kernel (abridged):\n",
    "```\n",
    "define amdgpu_kernel void @add_kernel(\n",
    "    ptr addrspace(1) nocapture readonly %0, \n",
    "    ptr addrspace(1) nocapture readonly %1, \n",
    "    ptr addrspace(1) nocapture writeonly %2\n",
    "  )\n",
    " \n",
    "local_unnamed_addr #0 !dbg !4 {\n",
    "  %4 = tail call i32 @llvm.amdgcn.workgroup.id.x(), !dbg !7\n",
    "  %5 = shl i32 %4, 10, !dbg !8\n",
    "  %6 = tail call i32 @llvm.amdgcn.workitem.id.x(), !dbg !9\n",
    "  %7 = shl i32 %6, 2, !dbg !9\n",
    "  %8 = and i32 %7, 1020, !dbg !9\n",
    "  %9 = or disjoint i32 %8, %5, !dbg !10\n",
    "  %10 = icmp slt i32 %9, 1024, !dbg !11\n",
    "  br i1 %10, label %.critedge, label %.critedge2, !dbg !12\n",
    "\n",
    ".critedge:                                        ; preds = %3\n",
    "  %11 = or disjoint i32 %9, 3, !dbg !10\n",
    "  %12 = or disjoint i32 %9, 2, !dbg !10\n",
    "  %13 = or disjoint i32 %9, 1, !dbg !10\n",
    "  %14 = sext i32 %9 to i64, !dbg !13\n",
    "  %15 = getelementptr float, ptr addrspace(1) %0, i64 %14, !dbg !13\n",
    "  %16 = addrspacecast ptr addrspace(1) %15 to ptr, !dbg !12\n",
    "  %17 = load float, ptr %16, align 16, !dbg !12\n",
    "  %18 = getelementptr inbounds i8, ptr %16, i64 4, !dbg !12\n",
    "  %19 = load float, ptr %18, align 4, !dbg !12\n",
    "  %20 = getelementptr inbounds i8, ptr %16, i64 8, !dbg !12\n",
    "  %21 = load float, ptr %20, align 8, !dbg !12\n",
    "  %22 = getelementptr inbounds i8, ptr %16, i64 12, !dbg !12\n",
    "  %23 = load float, ptr %22, align 4, !dbg !12\n",
    "  %24 = getelementptr float, ptr addrspace(1) %1, i64 %14, !dbg !14\n",
    "  %25 = addrspacecast ptr addrspace(1) %24 to ptr, !dbg !15\n",
    "  %26 = sext i32 %11 to i64, !dbg !16\n",
    "  %27 = getelementptr float, ptr addrspace(1) %2, i64 %26, !dbg !16\n",
    "  %28 = sext i32 %12 to i64, !dbg !16\n",
    "  %29 = getelementptr float, ptr addrspace(1) %2, i64 %28, !dbg !16\n",
    "  %30 = sext i32 %13 to i64, !dbg !16\n",
    "  %31 = getelementptr float, ptr addrspace(1) %2, i64 %30, !dbg !16\n",
    "  %32 = getelementptr inbounds i8, ptr %25, i64 12, !dbg !15\n",
    "  %33 = load float, ptr %32, align 4, !dbg !15\n",
    "  %34 = fadd float %23, %33, !dbg !17\n",
    "  %35 = getelementptr inbounds i8, ptr %25, i64 8, !dbg !15\n",
    "  %36 = load float, ptr %35, align 8, !dbg !15\n",
    "  %37 = fadd float %21, %36, !dbg !17\n",
    "  %38 = getelementptr inbounds i8, ptr %25, i64 4, !dbg !15\n",
    "  %39 = load float, ptr %38, align 4, !dbg !15\n",
    "  %40 = fadd float %19, %39, !dbg !17\n",
    "  %41 = load float, ptr %25, align 16, !dbg !15\n",
    "  %42 = fadd float %17, %41, !dbg !17\n",
    "  %43 = sext i32 %9 to i64, !dbg !16\n",
    "  %44 = getelementptr float, ptr addrspace(1) %2, i64 %43, !dbg !16\n",
    "  store float %42, ptr addrspace(1) %44, align 4, !dbg !18\n",
    "  store float %40, ptr addrspace(1) %31, align 4, !dbg !18\n",
    "  store float %37, ptr addrspace(1) %29, align 4, !dbg !18\n",
    "  store float %34, ptr addrspace(1) %27, align 4, !dbg !18\n",
    "  br label %.critedge2, !dbg !18\n",
    "\n",
    ".critedge2:                                       ; preds = %3, %.critedge\n",
    "  ret void, !dbg !19\n",
    "}\n",
    "\n",
    "attributes #0 = { mustprogress nofree norecurse nosync nounwind willreturn \n",
    "                  memory(argmem: readwrite) \n",
    "                  \"amdgpu-flat-work-group-size\"=\"1,256\" \n",
    "                  \"amdgpu-waves-per-eu\"=\"1\" \n",
    "                  \"denormal-fp-math-f32\"=\"ieee\"\n",
    "                }\n",
    "```\n",
    "\n",
    "### How to read this IR\n",
    "\n",
    "#### Header\n",
    "* Kernel entry:\n",
    "```\n",
    "define amdgpu_kernel void @add_kernel(\n",
    "    ptr addrspace(1) nocapture readonly %0,   ; x\n",
    "    ptr addrspace(1) nocapture readonly %1,   ; y\n",
    "    ptr addrspace(1) nocapture writeonly %2   ; out\n",
    ") \n",
    "```\n",
    "`addrspace(1)` is global Mem\n",
    "* **Attributes include**\n",
    "  * `\"amdgpu-flat-work-group-size\"=\"1,256\"` -> 256 threads per group (numwarps=4)\n",
    "  * `\"amdgpu-waves-per-eu\"=\"1\"` -> min 1 warp per Execution Unit ~ SIMD (occupancy hint) - not sure, I will verify this information later.\n",
    "\n",
    "#### Thread/Block indexing\n",
    "```\n",
    "  %4 = tail call i32 @llvm.amdgcn.workgroup.id.x(), !dbg !7   ; blockIdx.x\n",
    "  %5 = shl i32 %4, 10, !dbg !8                                ; base = blockIdx.x * 1024 \n",
    "  %6 = tail call i32 @llvm.amdgcn.workitem.id.x(), !dbg !9    ; threadIdx.x in range (0,255)\n",
    "  %7 = shl i32 %6, 2, !dbg !9                                 ; threadIdx.x * 4 (4 elements/thread)\n",
    "  %8 = and i32 %7, 1020, !dbg !9                              ; clamp to 1020\n",
    "  %9 = or disjoint i32 %8, %5, !dbg !10                       ; global index start for this thread [1024K + 0, 1024K + 4, ..., 1024K + 255 * 4]\n",
    "  %10 = icmp slt i32 %9, 1024, !dbg !11                       ; masking id < N (N=1024)\n",
    "  br i1 %10, label %.critedge, label %.critedge2, !dbg !12    ; Execution only does loads/stores when the mask is true\n",
    "```\n",
    "\n",
    "#### Addressing & loads\n",
    "\n",
    "```\n",
    "%14 = sext i32 %9 to i64\n",
    "%15 = getelementptr float, ptr addrspace(1) %0, i64 %14   ; &x[i]\n",
    "%16 = addrspacecast ptr addrspace(1) %15 to ptr           ; cast to flat for byte GEPs\n",
    "\n",
    "; Load 4 consecutive floats from x:\n",
    "%17 = load float, ptr %16,        align 16    ; x[i]\n",
    "%18 = getelementptr i8, ptr %16, i64 4\n",
    "%19 = load float, ptr %18,        align 4     ; x[i+1]\n",
    "%20 = getelementptr i8, ptr %16, i64 8\n",
    "%21 = load float, ptr %20,        align 8     ; x[i+2]\n",
    "%22 = getelementptr i8, ptr %16, i64 12\n",
    "%23 = load float, ptr %22,        align 4     ; x[i+3]\n",
    "\n",
    "; Load 4 consecutive floats from y:\n",
    "%24 = getelementptr float, ptr addrspace(1) %1, i64 %14   ; &y[i] in AS1\n",
    "%25 = addrspacecast ptr addrspace(1) %24 to ptr\n",
    "%41 = load float, ptr %25,        align 16    ; y[i]\n",
    "%39 = load float, ptr %38,        align 4     ; y[i+1]\n",
    "%36 = load float, ptr %35,        align 8     ; y[i+2]\n",
    "%33 = load float, ptr %32,        align 4     ; y[i+3]\n",
    "```\n",
    "\n",
    "#### Adds and stores\n",
    "\n",
    "```\n",
    "%42 = fadd float %17, %41   ; x[i]   + y[i]\n",
    "%40 = fadd float %19, %39   ; x[i+1] + y[i+1]\n",
    "%37 = fadd float %21, %36   ; x[i+2] + y[i+2]\n",
    "%34 = fadd float %23, %33   ; x[i+3] + y[i+3]\n",
    "\n",
    "; Compute output addresses (again using lane indices {i, i+1, i+2, i+3}) and store:\n",
    "%43 = sext i32 %9  to i64\n",
    "%44 = getelementptr float, ptr addrspace(1) %2, i64 %43   ; &out[i]\n",
    "store float %42, ptr addrspace(1) %44, align 4\n",
    "\n",
    "; %31 -> &out[i+1], %29 -> &out[i+2], %27 -> &out[i+3]\n",
    "store float %40, ptr addrspace(1) %31, align 4\n",
    "store float %37, ptr addrspace(1) %29, align 4\n",
    "store float %34, ptr addrspace(1) %27, align 4\n",
    "```\n",
    "\n",
    "After LLVM IR, Triton (via the backend toolchain) lowers to device assembly and then to a loadable binary:\n",
    "\n",
    "* **NVIDIA**: LLVM → PTX → `ptxas` → **cubin**  \n",
    "* **AMD**: LLVM → AMDGCN → linker → **hsaco**\n",
    "\n",
    "At this point the kernel is ready to run. For most kernel-level optimization, understanding LLVM IR is usually sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3b1386",
   "metadata": {},
   "source": [
    "# Reference\n",
    "1. [Triton kernel compilation stages by Pytorch](https://pytorch.org/blog/triton-kernel-compilation-stages/)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
